{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb795b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in e:\\data science files\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\data science files\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\data science files\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\data science files\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\data science files\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92c610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Beautifulsoup4 in e:\\data science files\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\data science files\\lib\\site-packages (from Beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4978efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a89341d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8393761",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd9531e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325e2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2267cf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: \"Baby Shark Dance\"[7]\n",
      "Name: Pinkfong Baby Shark - Kids' Songs & Stories\n",
      "Artist: 14.32\n",
      "Upload: June 17, 2016\n",
      "\n",
      "Rank: \"Despacito\"[10]\n",
      "Name: Luis Fonsi\n",
      "Artist: 8.41\n",
      "Upload: January 12, 2017\n",
      "\n",
      "Rank: \"Johny Johny Yes Papa\"[18]\n",
      "Name: LooLoo Kids - Nursery Rhymes and Children's Songs\n",
      "Artist: 6.89\n",
      "Upload: October 8, 2016\n",
      "\n",
      "Rank: \"Bath Song\"[19]\n",
      "Name: Cocomelon - Nursery Rhymes\n",
      "Artist: 6.66\n",
      "Upload: May 2, 2018\n",
      "\n",
      "Rank: \"Shape of You\"[20]\n",
      "Name: Ed Sheeran\n",
      "Artist: 6.23\n",
      "Upload: January 30, 2017\n",
      "\n",
      "Rank: \"See You Again\"[23]\n",
      "Name: Wiz Khalifa\n",
      "Artist: 6.22\n",
      "Upload: April 6, 2015\n",
      "\n",
      "Rank: \"Wheels on the Bus\"[28]\n",
      "Name: Cocomelon - Nursery Rhymes\n",
      "Artist: 6.01\n",
      "Upload: May 24, 2018\n",
      "\n",
      "Rank: \"Phonics Song with Two Words\"[29]\n",
      "Name: ChuChu TV Nursery Rhymes & Kids Songs\n",
      "Artist: 5.75\n",
      "Upload: March 6, 2014\n",
      "\n",
      "Rank: \"Uptown Funk\"[30]\n",
      "Name: Mark Ronson\n",
      "Artist: 5.18\n",
      "Upload: November 19, 2014\n",
      "\n",
      "Rank: \"Gangnam Style\"[31]\n",
      "Name: Psy\n",
      "Artist: 5.10\n",
      "Upload: July 15, 2012\n",
      "\n",
      "Rank: \"Learning Colors – Colorful Eggs on a Farm\"[36]\n",
      "Name: Miroshka TV\n",
      "Artist: 5.09\n",
      "Upload: February 27, 2018\n",
      "\n",
      "Rank: \"Dame Tu Cosita\"[37]\n",
      "Name: Ultra Records\n",
      "Artist: 4.59\n",
      "Upload: April 5, 2018\n",
      "\n",
      "Rank: \"Masha and the Bear – Recipe for Disaster\"[38]\n",
      "Name: Get Movies\n",
      "Artist: 4.57\n",
      "Upload: January 31, 2012\n",
      "\n",
      "Rank: \"Axel F\"[39]\n",
      "Name: Crazy Frog\n",
      "Artist: 4.45\n",
      "Upload: June 16, 2009\n",
      "\n",
      "Rank: \"Sugar\"[40]\n",
      "Name: Maroon 5\n",
      "Artist: 4.02\n",
      "Upload: January 14, 2015\n",
      "\n",
      "Rank: \"Baa Baa Black Sheep\"[41]\n",
      "Name: Cocomelon - Nursery Rhymes\n",
      "Artist: 4.01\n",
      "Upload: June 25, 2018\n",
      "\n",
      "Rank: \"Counting Stars\"[42]\n",
      "Name: OneRepublic\n",
      "Artist: 4.00\n",
      "Upload: May 31, 2013\n",
      "\n",
      "Rank: \"Lakdi Ki Kathi\"[43]\n",
      "Name: Jingle Toons\n",
      "Artist: 3.98\n",
      "Upload: June 14, 2018\n",
      "\n",
      "Rank: \"Roar\"[44]\n",
      "Name: Katy Perry\n",
      "Artist: 3.98\n",
      "Upload: September 5, 2013\n",
      "\n",
      "Rank: \"Waka Waka (This Time for Africa)\"[45]\n",
      "Name: Shakira\n",
      "Artist: 3.89\n",
      "Upload: June 4, 2010\n",
      "\n",
      "Rank: \"Sorry\"[46]\n",
      "Name: Justin Bieber\n",
      "Artist: 3.78\n",
      "Upload: October 22, 2015\n",
      "\n",
      "Rank: \"Shree Hanuman Chalisa\"[47]\n",
      "Name: T-Series Bhakti Sagar\n",
      "Artist: 3.77\n",
      "Upload: May 10, 2011\n",
      "\n",
      "Rank: \"Humpty the train on a fruits ride\"[48]\n",
      "Name: Kiddiestv Hindi - Nursery Rhymes & Kids Songs\n",
      "Artist: 3.76\n",
      "Upload: January 26, 2018\n",
      "\n",
      "Rank: \"Thinking Out Loud\"[49]\n",
      "Name: Ed Sheeran\n",
      "Artist: 3.75\n",
      "Upload: October 7, 2014\n",
      "\n",
      "Rank: \"Perfect\"[50]\n",
      "Name: Ed Sheeran\n",
      "Artist: 3.70\n",
      "Upload: November 9, 2017\n",
      "\n",
      "Rank: \"Dark Horse\"[51]\n",
      "Name: Katy Perry\n",
      "Artist: 3.70\n",
      "Upload: February 20, 2014\n",
      "\n",
      "Rank: \"Let Her Go\"[52]\n",
      "Name: Passenger\n",
      "Artist: 3.64\n",
      "Upload: July 25, 2012\n",
      "\n",
      "Rank: \"Faded\"[53]\n",
      "Name: Alan Walker\n",
      "Artist: 3.60\n",
      "Upload: December 3, 2015\n",
      "\n",
      "Rank: \"Girls Like You\"[54]\n",
      "Name: Maroon 5\n",
      "Artist: 3.58\n",
      "Upload: May 31, 2018\n",
      "\n",
      "Rank: \"Lean On\"[55]\n",
      "Name: Major Lazer Official\n",
      "Artist: 3.57\n",
      "Upload: March 22, 2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "names = []\n",
    "artists = []\n",
    "uploads = []\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    # Extract data from each column in the row\n",
    "    data = row.find_all(\"td\")\n",
    "    \n",
    "    try:\n",
    "        # Extract and append the details to the respective lists\n",
    "        ranks.append(data[0].text.strip())\n",
    "        names.append(data[1].text.strip())\n",
    "        artists.append(data[2].text.strip())\n",
    "        uploads.append(data[3].text.strip())\n",
    "    except IndexError:\n",
    "        # If there are fewer elements in the row than expected, skip this row\n",
    "        continue\n",
    "\n",
    "# Print the extracted details\n",
    "for rank, name, artist, upload in zip(ranks, names, artists, uploads):\n",
    "    print(\"Rank:\", rank)\n",
    "    print(\"Name:\", name)\n",
    "    print(\"Artist:\", artist)\n",
    "    print(\"Upload:\", upload)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4332cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e839056",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bcci.tv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08769302",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "004f3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eaca5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixture_link = soup.find(\"a\", string=\"International Fixtures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52e6f18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "International Fixtures link not found.\n"
     ]
    }
   ],
   "source": [
    "if fixture_link:\n",
    "    # Construct the full URL for the international fixtures page\n",
    "    fixture_url = url + fixture_link[\"href\"]\n",
    "\n",
    "    # Send a GET request to the international fixtures page\n",
    "    response = requests.get(fixture_url)\n",
    "\n",
    "    # Parse the HTML content of the international fixtures page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all the fixtures\n",
    "    fixtures = soup.find_all(\"div\", class_=\"fixture__info u-skewed\")\n",
    "\n",
    "    # Extract and print the details of each fixture\n",
    "    for fixture in fixtures:\n",
    "        series = fixture.find(\"span\", class_=\"u-unskewed-text\").text.strip()\n",
    "        place = fixture.find(\"p\", class_=\"fixture__additional-info\").text.strip()\n",
    "        date = fixture.find(\"div\", class_=\"fixture__full-date\").text.strip()\n",
    "        time = fixture.find(\"span\", class_=\"fixture__time\").text.strip()\n",
    "\n",
    "        print(\"Series:\", series)\n",
    "        print(\"Place:\", place)\n",
    "        print(\"Date:\", date)\n",
    "        print(\"Time:\", time)\n",
    "        print()\n",
    "else:\n",
    "    print(\"International Fixtures link not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details: A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff239cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economy link not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the economy page\n",
    "economy_link = soup.find(\"a\", string=\"Economy\")\n",
    "\n",
    "# Check if the economy link is found\n",
    "if economy_link:\n",
    "    # Construct the full URL for the economy page\n",
    "    economy_url = url + economy_link[\"href\"]\n",
    "\n",
    "    # Send a GET request to the economy page\n",
    "    response = requests.get(economy_url)\n",
    "\n",
    "    # Parse the HTML content of the economy page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the link to the GDP of Indian states page\n",
    "    gdp_link = soup.find(\"a\", string=\"GDP of Indian states\")\n",
    "\n",
    "    # Check if the GDP link is found\n",
    "    if gdp_link:\n",
    "        # Construct the full URL for the GDP of Indian states page\n",
    "        gdp_url = url + gdp_link[\"href\"]\n",
    "\n",
    "        # Send a GET request to the GDP of Indian states page\n",
    "        response = requests.get(gdp_url)\n",
    "\n",
    "        # Parse the HTML content of the GDP of Indian states page\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find the table containing the state-wise GDP details\n",
    "        table = soup.find(\"table\", class_=\"display\")\n",
    "\n",
    "        # Extract and print the details of each state\n",
    "        for row in table.find_all(\"tr\")[1:]:\n",
    "            columns = row.find_all(\"td\")\n",
    "            rank = columns[0].text.strip()\n",
    "            state = columns[1].text.strip()\n",
    "            gsdp_1819 = columns[2].text.strip()\n",
    "            gsdp_1920 = columns[3].text.strip()\n",
    "            share_1819 = columns[4].text.strip()\n",
    "            gdp_billion = columns[5].text.strip()\n",
    "\n",
    "            print(\"Rank:\", rank)\n",
    "            print(\"State:\", state)\n",
    "            print(\"GSDP(18-19) - Current Prices:\", gsdp_1819)\n",
    "            print(\"GSDP(19-20) - Current Prices:\", gsdp_1920)\n",
    "            print(\"Share(18-19):\", share_1819)\n",
    "            print(\"GDP ($ Billion):\", gdp_billion)\n",
    "            print()\n",
    "\n",
    "    else:\n",
    "        print(\"GDP of Indian states link not found.\")\n",
    "else:\n",
    "    print(\"Economy link not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c47984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "034c1e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore menu not found.\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "# GitHub homepage URL\n",
    "homepage_url = \"https://github.com/\"\n",
    "\n",
    "# Send a GET request to the GitHub homepage\n",
    "homepage_response = session.get(homepage_url)\n",
    "\n",
    "# Parse the HTML content of the homepage\n",
    "homepage_soup = BeautifulSoup(homepage_response.content, \"html.parser\")\n",
    "\n",
    "# Find the Explore menu\n",
    "explore_menu = homepage_soup.find(\"nav\", class_=\"mt-0 px-3 px-lg-0 mb-5 mb-lg-0\")\n",
    "\n",
    "# Check if the Explore menu is found\n",
    "if explore_menu:\n",
    "    # Find the URL of the trending page from the Explore menu\n",
    "    trending_link = explore_menu.find(\"a\", text=\"Trending\")\n",
    "\n",
    "    # Check if the trending link is found\n",
    "    if trending_link:\n",
    "        # Get the href attribute value of the trending link\n",
    "        trending_url = trending_link[\"href\"]\n",
    "\n",
    "        # Construct the full URL for the trending page\n",
    "        trending_full_url = homepage_url + trending_url\n",
    "\n",
    "        # Send a GET request to the trending page\n",
    "        trending_response = session.get(trending_full_url)\n",
    "\n",
    "        # Parse the HTML content of the trending page\n",
    "        trending_soup = BeautifulSoup(trending_response.content, \"html.parser\")\n",
    "\n",
    "        # Find all the repository cards\n",
    "        repository_cards = trending_soup.find_all(\"article\", class_=\"Box-row\")\n",
    "\n",
    "        # Extract and print the details of each repository\n",
    "        for card in repository_cards:\n",
    "            # Repository title\n",
    "            title = card.find(\"h1\", class_=\"h3\").text.strip()\n",
    "\n",
    "            # Repository description\n",
    "            description = card.find(\"p\", class_=\"col-9\").text.strip()\n",
    "\n",
    "            # Contributors count\n",
    "            contributors_count = card.find(\"a\", class_=\"muted-link\").text.strip()\n",
    "\n",
    "            # Language used\n",
    "            language = card.find(\"span\", itemprop=\"programmingLanguage\")\n",
    "            if language:\n",
    "                language = language.text.strip()\n",
    "            else:\n",
    "                language = \"Not specified\"\n",
    "\n",
    "            print(\"Repository Title:\", title)\n",
    "            print(\"Repository Description:\", description)\n",
    "            print(\"Contributors Count:\", contributors_count)\n",
    "            print(\"Language Used:\", language)\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Trending link not found.\")\n",
    "else:\n",
    "    print(\"Explore menu not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3d1555c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charts option not found.\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "# Billboard homepage URL\n",
    "homepage_url = \"https://www.billboard.com/\"\n",
    "\n",
    "# Send a GET request to the Billboard homepage\n",
    "homepage_response = session.get(homepage_url)\n",
    "\n",
    "# Parse the HTML content of the homepage\n",
    "homepage_soup = BeautifulSoup(homepage_response.content, \"html.parser\")\n",
    "\n",
    "# Find the URL of the hot 100-page link from the charts option\n",
    "charts_option = homepage_soup.find(\"a\", class_=\"header__main-link\", string=\"Charts\")\n",
    "\n",
    "# Check if the charts option is found\n",
    "if charts_option:\n",
    "    # Get the href attribute value of the hot 100-page link\n",
    "    hot100_link = charts_option['href']\n",
    "\n",
    "    # Construct the full URL for the hot 100-page\n",
    "    hot100_full_url = homepage_url + hot100_link\n",
    "\n",
    "    # Send a GET request to the hot 100-page\n",
    "    hot100_response = session.get(hot100_full_url)\n",
    "\n",
    "    # Parse the HTML content of the hot 100-page\n",
    "    hot100_soup = BeautifulSoup(hot100_response.content, \"html.parser\")\n",
    "\n",
    "    # Find all the song rows\n",
    "    song_rows = hot100_soup.find_all(\"li\", class_=\"chart-list__element display--flex\")\n",
    "\n",
    "    # Extract and print the details of each song\n",
    "    for row in song_rows:\n",
    "        # Song name\n",
    "        song_name = row.find(\"span\", class_=\"chart-element__information__song text--truncate color--primary\").text.strip()\n",
    "\n",
    "        # Artist name\n",
    "        artist_name = row.find(\"span\", class_=\"chart-element__information__artist text--truncate color--secondary\").text.strip()\n",
    "\n",
    "        # Last week rank\n",
    "        last_week_rank = row.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--last\").text.strip()\n",
    "\n",
    "        # Peak rank\n",
    "        peak_rank = row.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--peak\").text.strip()\n",
    "\n",
    "        # Weeks on board\n",
    "        weeks_on_board = row.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--week\").text.strip()\n",
    "\n",
    "        print(\"Song Name:\", song_name)\n",
    "        print(\"Artist Name:\", artist_name)\n",
    "        print(\"Last Week Rank:\", last_week_rank)\n",
    "        print(\"Peak Rank:\", peak_rank)\n",
    "        print(\"Weeks on Board:\", weeks_on_board)\n",
    "        print()\n",
    "\n",
    "else:\n",
    "    print(\"Charts option not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab648bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6.Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    "Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "029857e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charts option not found.\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "# Billboard homepage URL\n",
    "homepage_url = \"https://www.billboard.com/\"\n",
    "\n",
    "# Send a GET request to the Billboard homepage\n",
    "homepage_response = session.get(homepage_url)\n",
    "\n",
    "# Parse the HTML content of the homepage\n",
    "homepage_soup = BeautifulSoup(homepage_response.content, \"html.parser\")\n",
    "\n",
    "# Find the URL of the hot 100-page link from the charts option\n",
    "charts_option = homepage_soup.find(\"a\", class_=\"header__main-link\", string=\"Charts\")\n",
    "\n",
    "# Check if the charts option is found\n",
    "if charts_option:\n",
    "    # Get the href attribute value of the hot 100-page link\n",
    "    hot100_link = charts_option['href']\n",
    "\n",
    "    # Construct the full URL for the hot 100-page\n",
    "    hot100_full_url = homepage_url + hot100_link\n",
    "\n",
    "    # Send a GET request to the hot 100-page\n",
    "    hot100_response = session.get(hot100_full_url)\n",
    "\n",
    "    # Parse the HTML content of the hot 100-page\n",
    "    hot100_soup = BeautifulSoup(hot100_response.content, \"html.parser\")\n",
    "\n",
    "    # Find all the song rows\n",
    "    song_rows = hot100_soup.find_all(\"li\", class_=\"chart-list__element display--flex\")\n",
    "\n",
    "    # Extract and print the details of each song\n",
    "    for row in song_rows:\n",
    "        # Song name\n",
    "        song_name = row.find(\"span\", class_=\"chart-element__information__song text--truncate color--primary\").text.strip()\n",
    "\n",
    "        # Artist name\n",
    "        artist_name = row.find(\"span\", class_=\"chart-element__information__artist text--truncate color--secondary\").text.strip()\n",
    "\n",
    "        # Last week rank\n",
    "        last_week_rank = row.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--last\").text.strip()\n",
    "\n",
    "        # Peak rank\n",
    "        peak_rank = row.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--peak\").text.strip()\n",
    "\n",
    "        # Weeks on board\n",
    "        weeks_on_board = row.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--week\").text.strip()\n",
    "\n",
    "        print(\"Song Name:\", song_name)\n",
    "        print(\"Artist Name:\", artist_name)\n",
    "        print(\"Last Week Rank:\", last_week_rank)\n",
    "        print(\"Peak Rank:\", peak_rank)\n",
    "        print(\"Weeks on Board:\", weeks_on_board)\n",
    "        print()\n",
    "\n",
    "else:\n",
    "    print(\"Charts option not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45bf8c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# IMDb URL for the list of most watched TV series of all time\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "# Send a GET request to the IMDb URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the TV series items\n",
    "tv_series_items = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "# Iterate over each TV series item and extract the details\n",
    "for item in tv_series_items:\n",
    "    # Name\n",
    "    name = item.find(\"h3\", class_=\"lister-item-header\").a.text.strip()\n",
    "\n",
    "    # Year span\n",
    "    year_span = item.find(\"span\", class_=\"lister-item-year\").text.strip()\n",
    "\n",
    "    # Genre\n",
    "    genre = item.find(\"span\", class_=\"genre\").text.strip()\n",
    "\n",
    "    # Run time\n",
    "    runtime = item.find(\"span\", class_=\"runtime\").text.strip()\n",
    "\n",
    "    # Ratings\n",
    "    ratings = item.find(\"span\", class_=\"ipl-rating-star__rating\").text.strip()\n",
    "\n",
    "    # Votes\n",
    "    votes = item.find(\"span\", attrs={\"name\": \"nv\"}).text.strip()\n",
    "\n",
    "    print(\"Name:\", name)\n",
    "    print(\"Year Span:\", year_span)\n",
    "    print(\"Genre:\", genre)\n",
    "    print(\"Run Time:\", runtime)\n",
    "    print(\"Ratings:\", ratings)\n",
    "    print(\"Votes:\", votes)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8 Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code.\n",
    "ASSIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a892e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to Show All Dataset page not found.\n"
     ]
    }
   ],
   "source": [
    "# Find the link to the Show All Dataset page\n",
    "all_datasets_link = soup.find(\"a\", string=\"View ALL Data Sets\")\n",
    "\n",
    "# Check if the link is found\n",
    "if all_datasets_link:\n",
    "    # Construct the full URL for the Show All Dataset page\n",
    "    all_datasets_url = url + all_datasets_link.get(\"href\")\n",
    "\n",
    "    # Send a GET request to the Show All Dataset page\n",
    "    datasets_response = requests.get(all_datasets_url)\n",
    "\n",
    "    # Parse the HTML content of the Show All Dataset page\n",
    "    datasets_soup = BeautifulSoup(datasets_response.content, \"html.parser\")\n",
    "\n",
    "    # Find all dataset rows\n",
    "    dataset_rows = datasets_soup.find_all(\"tr\", bgcolor=\"#FFFFFF\")\n",
    "\n",
    "    # Iterate over each dataset row and extract the details\n",
    "    for row in dataset_rows:\n",
    "        # Dataset name\n",
    "        dataset_name = row.find(\"a\").text.strip()\n",
    "\n",
    "        # Data type\n",
    "        data_type = row.find_all(\"td\")[1].text.strip()\n",
    "\n",
    "        # Task\n",
    "        task = row.find_all(\"td\")[2].text.strip()\n",
    "\n",
    "        # Attribute type\n",
    "        attribute_type = row.find_all(\"td\")[3].text.strip()\n",
    "\n",
    "        # Number of instances\n",
    "        num_instances = row.find_all(\"td\")[4].text.strip()\n",
    "\n",
    "        # Number of attributes\n",
    "        num_attributes = row.find_all(\"td\")[5].text.strip()\n",
    "\n",
    "        # Year\n",
    "        year = row.find_all(\"td\")[6].text.strip()\n",
    "\n",
    "        print(\"Dataset Name:\", dataset_name)\n",
    "        print(\"Data Type:\", data_type)\n",
    "        print(\"Task:\", task)\n",
    "        print(\"Attribute Type:\", attribute_type)\n",
    "        print(\"Number of Instances:\", num_instances)\n",
    "        print(\"Number of Attributes:\", num_attributes)\n",
    "        print(\"Year:\", year)\n",
    "        print()\n",
    "else:\n",
    "    print(\"Link to Show All Dataset page not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c46da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
