{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c514dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ef4a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in e:\\data science files\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\data science files\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0dc2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00b68ea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2257105227.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    movies_data = [https://www.imdb.com/list/ls056092300/]\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def scrape_imdb_top_100_indian_movies(url):\n",
    "        response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        movies_data = [https://www.imdb.com/list/ls056092300/]\n",
    "\n",
    "        for movie_tag in soup.find_all('div', class_='lister-item-content'):\n",
    "            name = movie_tag.find('a').text.strip()\n",
    "            rating = movie_tag.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "            year = movie_tag.find('span', class_='lister-item-year').text.strip('()')\n",
    "\n",
    "            movies_data.append({\n",
    "                'Name': name,\n",
    "                'Rating': rating,\n",
    "                'Year of Release': year\n",
    "            })\n",
    "\n",
    "        return movies_data\n",
    "    \n",
    "else:\n",
    "        print(f\"Failed to fetch data from {https://www.imdb.com/list/ls056092300/}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a8b9a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name Rating Year of Release\n",
      "0                     Ship of Theseus      8            2012\n",
      "1                              Iruvar    8.4            1997\n",
      "2                     Kaagaz Ke Phool    7.8            1959\n",
      "3   Lagaan: Once Upon a Time in India    8.1            2001\n",
      "4                     Pather Panchali    8.2            1955\n",
      "..                                ...    ...             ...\n",
      "95                        Apur Sansar    8.4            1959\n",
      "96                        Kanchivaram    8.2            2008\n",
      "97                    Monsoon Wedding    7.3            2001\n",
      "98                              Black    8.1            2005\n",
      "99                            Deewaar      8            1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_imdb_top_100_indian_movies(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        movies_data = []\n",
    "\n",
    "        for movie_tag in soup.find_all('div', class_='lister-item-content'):\n",
    "            name = movie_tag.find('a').text.strip()\n",
    "            rating = movie_tag.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "            year = movie_tag.find('span', class_='lister-item-year').text.strip('()')\n",
    "\n",
    "            movies_data.append({\n",
    "                'Name': name,\n",
    "                'Rating': rating,\n",
    "                'Year of Release': year\n",
    "            })\n",
    "\n",
    "        return movies_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "        return None\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "\n",
    "movies_data = scrape_imdb_top_100_indian_movies(url)\n",
    "\n",
    "if movies_data:\n",
    "    df = pd.DataFrame(movies_data)\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Data scraping unsuccessful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0cf2357d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:34\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape product name, price and discounts from https://peachmode.com/search?q=bags\n",
    "def scrape_peachmode_products(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "        if response.\n",
    "        \n",
    "         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            products = []\n",
    "        for product_card in soup.find_all('div', class_='product-grid-item'):\n",
    "            product_name = product_card.find('h4', class_='product-title').text.strip()\n",
    "            product_price = product_card.find('span', class_='product-price').text.strip()\n",
    "            product_discount = product_card.find('span', class_='product-discount').text.strip()\n",
    "\n",
    "            products.append({\n",
    "                'name': product_name,\n",
    "                'price': product_price,\n",
    "                'discount': product_discount,\n",
    "            })\n",
    "\n",
    "        return products\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(f\"Error: Unable to fetch data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "            print(f\"Name: {product['name']}\")\n",
    "            print(f\"Price: {product['price']}\")\n",
    "            print(f\"Discount: {product['discount']}\")\n",
    "    else:\n",
    "        print(\"No data to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a68e6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data to display.\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape product name, price and discounts from https://peachmode.com/search?q=bags\n",
    "def scrape_peachmode_products(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        products = []\n",
    "        for card in soup.find_all('div', class_='grid'):\n",
    "            name = product_card.find('h4', class_='title').text.strip()\n",
    "            price = product_card.find('span', class_='price').text.strip()\n",
    "            discount = product_card.find('span', class_='discount').text.strip()\n",
    "\n",
    "            products.append({\n",
    "                'name': p_name,\n",
    "                'price': p_price,\n",
    "                'discount': p_discount,\n",
    "            })\n",
    "\n",
    "        return products\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(f\"Error: Unable to fetch data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    target_url = \"https://peachmode.com/search?q=bags\"\n",
    "\n",
    "    scraped_data = scrape_peachmode_products(target_url)\n",
    "\n",
    "    if scraped_data:\n",
    "        for index, product in enumerate(scraped_data, start=1):\n",
    "            print(f\"\\nProduct {index}:\")\n",
    "            print(f\"Name: {product['name']}\")\n",
    "            print(f\"Price: {product['price']}\")\n",
    "            print(f\"Discount: {product['discount']}\")\n",
    "    else:\n",
    "        print(\"No data to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a56aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: a) Top 10 ODI teams in menâ€™s cricket along with the records for matches, points and rating. \n",
    "\n",
    "def scrape_odi_teams():\n",
    "    \n",
    "    teams_data = []\n",
    "\n",
    "    for team in soup.find_all('tr', class_='rankings-block__banner')[:1]:\n",
    "        name = team.find('span', class_='u-hide-phablet').text.strip()\n",
    "        match = team.find('td', class_='rankings-block__banner--matches').text.strip()\n",
    "        point = team.find('td', class_='rankings-block__banner--points').text.strip()\n",
    "        rating = team.find('td', class_='rankings-block__banner--rating').text.strip()\n",
    "\n",
    "        teams_data.append({'Team': name, 'Matches': match, 'Points': point, 'Rating': rating})\n",
    "\n",
    "    for team in soup.find_all('tr', class_='table-body')[:9]:\n",
    "        name = team.find('span', class_='u-hide-phablet').text.strip()\n",
    "        match = team.find_all('td')[2].text.strip()\n",
    "        point = team.find_all('td')[3].text.strip()\n",
    "        rating = team.find_all('td')[4].text.strip()\n",
    "\n",
    "        teams_data.append({'Team': name, 'Matches': match, 'Points': point, 'Rating': rating})\n",
    "\n",
    "    df = pd.DataFrame(teams_data)\n",
    "    print(\"top ten ODI teams in men's cricket:\")\n",
    "    print df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75bfdb7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (747727223.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install youtube_dl\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install youtube_dl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def scrape_odi_teams():\n",
    "    url = \"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    teams_data = []\n",
    "\n",
    "    for team in soup.find_all('tr', class_='rankings-block__banner')[:1]:\n",
    "        name = team.find('span', class_='u-hide-phablet').text.strip()\n",
    "        match = team.find('td', class_='rankings-block__banner--matches').text.strip()\n",
    "        point = team.find('td', class_='rankings-block__banner--points').text.strip()\n",
    "        rating = team.find('td', class_='rankings-block__banner--rating').text.strip()\n",
    "\n",
    "        teams_data.append({'Team': name, 'Matches': match, 'Points': point, 'Rating': rating})\n",
    "\n",
    "    for team in soup.find_all('tr', class_='table-body')[:9]:\n",
    "        name = team.find('span', class_='u-hide-phablet').text.strip()\n",
    "        match = team.find_all('td')[2].text.strip()\n",
    "        point = team.find_all('td')[3].text.strip()\n",
    "        rating = team.find_all('td')[4].text.strip()\n",
    "\n",
    "        teams_data.append({'Team': name, 'Matches': match, 'Points': point, 'Rating': rating})\n",
    "\n",
    "    df = pd.DataFrame(teams_data)\n",
    "    print(\"Top ten ODI teams in men's cricket:\")\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_odi_teams()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440f7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the heading, date, content and the likes for the video from the link for the youtube video from the post.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import youtube_dl\n",
    "\n",
    "def scrape_patreon_post(url):\n",
    "    # Fetch HTML content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract post details\n",
    "    heading = soup.find('h1', class_='post-meta-title').text.strip()\n",
    "    date = soup.find('time', class_='post-meta-published').text.strip()\n",
    "    content = soup.find('div', class_='post-content').text.strip()\n",
    "\n",
    "    # Extract YouTube video link\n",
    "    youtube_link = soup.find('a', class_='oembed-link')['href']\n",
    "\n",
    "    # Get YouTube video details using youtube_dl\n",
    "    video_info = get_youtube_video_info(youtube_link)\n",
    "\n",
    "    # Extract likes from video details\n",
    "    likes = video_info.get('like_count', 'N/A')\n",
    "\n",
    "    # Print the details\n",
    "    print(f\"Heading: {heading}\")\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Content: {content}\")\n",
    "    print(f\"Likes: {likes}\")\n",
    "\n",
    "def get_youtube_video_info(youtube_link):\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'extract_flat': True,\n",
    "    }\n",
    "\n",
    "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "        info_dict = ydl.extract_info(youtube_link, download=False)\n",
    "        return info_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    patreon_url = \"https://www.patreon.com/coreyms\"\n",
    "    scrape_patreon_post(patreon_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a14304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping house details for indiranagar\n",
      "Scraping house details for jayanagar\n",
      "Scraping house details for rajajinagar\n"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the heading, date, content and the likes for the video from the link for the youtube video from the post. \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_house_details(locality):\n",
    "    url = f\"https://www.nobroker.in/property/sale/{locality}/mumbai\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        houses = soup.find_all('div', class_='card')\n",
    "        \n",
    "        for house in houses:\n",
    "            title = house.find('h2', class_='heading-6').text.strip()\n",
    "            location = house.find('div', class_='nb__2CMjv').text.strip()\n",
    "            area = house.find('div', class_='nb__3oNyC').text.strip()\n",
    "            emi = house.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "            price = house.find('div', class_='heading-7').text.strip()\n",
    "\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Location: {location}\")\n",
    "            print(f\"Area: {area}\")\n",
    "            print(f\"EMI: {emi}\")\n",
    "            print(f\"Price: {price}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {locality}. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    localities = [\"indiranagar\", \"jayanagar\", \"rajajinagar\"]\n",
    "\n",
    "    for locality in localities:\n",
    "        print(f\"Scraping house details for {locality}\")\n",
    "        scrape_house_details(locality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07896d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping product 1 due to missing details.\n",
      "Skipping product 2 due to missing details.\n",
      "Skipping product 3 due to missing details.\n",
      "Skipping product 4 due to missing details.\n",
      "Skipping product 5 due to missing details.\n",
      "Skipping product 6 due to missing details.\n",
      "Skipping product 7 due to missing details.\n",
      "Skipping product 8 due to missing details.\n",
      "Skipping product 9 due to missing details.\n",
      "Skipping product 10 due to missing details.\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/bestseller?sort=popular .\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.find_all('div', class_='productCardBox')\n",
    "\n",
    "        for i, product in enumerate(products[:10], start=1):\n",
    "            product_name_tag = product.find('h3', class_='product-title')\n",
    "            price_tag = product.find('span', class_='discounted-price')\n",
    "            image_tag = product.find('img')\n",
    "\n",
    "            # Check if elements exist before accessing attributes\n",
    "            if product_name_tag and price_tag and image_tag:\n",
    "                product_name = product_name_tag.text.strip()\n",
    "                price = price_tag.text.strip()\n",
    "                image_url = image_tag['src']\n",
    "\n",
    "                print(f\"Product {i}:\")\n",
    "                print(f\"Name: {product_name}\")\n",
    "                print(f\"Price: {price}\")\n",
    "                print(f\"Image URL: {image_url}\")\n",
    "                print(\"-\" * 50)\n",
    "            else:\n",
    "                print(f\"Skipping product {i} due to missing details.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "    scrape_product_details(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6e8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1-Please visit https://www.cnbc.com/world/?region=world and scrap-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_cnbc_world_news(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Scraping headings, dates, and news links\n",
    "        headings = [heading.text.strip() for heading in soup.find_all('h3', class_='Card-title')]\n",
    "        dates = [date.text.strip() for date in soup.find_all('div', class_='Card-time')]\n",
    "        news_links = [link['href'] for link in soup.select('.Card-title a[href]')]\n",
    "\n",
    "        # Print the scraped data\n",
    "        for i, (heading, date, link) in enumerate(zip(headings, dates, news_links), start=1):\n",
    "            print(f\"News {i}:\")\n",
    "            print(f\"Heading: {heading}\")\n",
    "            print(f\"Date: {date}\")\n",
    "            print(f\"News Link: {link}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.cnbc.com/world/?region=world\"\n",
    "    scrape_cnbc_world_news(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d395633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2-Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/ and scrap-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Scraping paper titles, dates, and authors\n",
    "        titles = [title.text.strip() for title in soup.find_all('h5', class_='media-heading')]\n",
    "        dates = [date.text.strip() for date in soup.find_all('span', class_='date')]\n",
    "        authors = [author.text.strip() for author in soup.find_all('span', class_='author')]\n",
    "\n",
    "        # Print the scraped data\n",
    "        for i, (title, date, author) in enumerate(zip(titles, dates, authors), start=1):\n",
    "            print(f\"Article {i}:\")\n",
    "            print(f\"Paper Title: {title}\")\n",
    "            print(f\"Date: {date}\")\n",
    "            print(f\"Author: {author}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\"\n",
    "    scrape_most_downloaded_articles(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113a74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8888/notebooks/Assignment-March%2024-Copy1.ipynb\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"downloaded_file.txt\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5e0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
